# ğŸ“˜ RAG-APP â€” Phase 2: Agentic RAG with LangGraph

RAG-APP Phase 2 is an advanced upgrade of the Phase 1 RAG system, introducing an Agentic Architecture powered by LangGraph.

---

**In this phase, the LLM autonomously decides:**

 - When to retrieve document context (tool-calling)

 - When a query is general (no retrieval needed)

 - How to combine session memory + document chunks

 - How to construct final answers via a multi-node workflow

This results in faster, smarter, and more context-aware interactions.

---

# ğŸš€ Whatâ€™s New in Phase 2?

| Feature |	Phase 1	| Phase 2 (New!) |
|------|--------------|-------------|
| **RAG Pipeline** | Static pipeline | Agentic graph with autonomous routing |
| **Tool Use** | None	| LangGraph ToolNode triggers rag_tool |
| **LLM Routing** | Always retrieval | LLM decides retrieval vs general answer |
| **Conversation Memory** |	Basic sliding window | Fully integrated in agent graph |
| **Architecture** | Linear	| Multi-node agent workflow |
| **Performance** | Redundant retrieval	| Retrieval only when needed |

---

# ğŸ§  Agentic Workflow Overview
```bash
START
  â†“
assistant_node  â†’  decides â†’ general OR rag_tool
  â”œâ”€â”€ tool_call â†’ tool_node â†’ finalize_node â†’ END(Final Response Without The Citations)
  â””â”€â”€ NO_TOOL_REQUIRED â†’ finalize_node
                              â†“
                             END(Final Response With The Citations)
```
---

**assistant_node**
 - LLM analyzes the query
 - If document-based â†’ produces a tool_call
 - If general â†’ routes to finalize_node

**tool_node (rag_tool)**
 - Retrieves top-k document chunks
 - Returns chunks + citations to graph

**finalize_node**
- Combines:
  - session memory
  - user question
  - retrieved chunks (if any)
  - Produces final answer

---

# ğŸ“ Project Structure (Phase 2)
```bash
RAG-APP/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ upload.py
â”‚   â”‚   â”‚   â”œâ”€â”€ process.py
â”‚   â”‚   â”‚   â”œâ”€â”€ query.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_tool.py
â”‚   â”‚   â”‚   â”œâ”€â”€ reset_session.py
â”‚   â”‚   â”‚   â””â”€â”€ list_docs.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py
â”‚   â”‚   â”‚   â”œâ”€â”€ citation_handler.py
â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_engine.py
â”‚   â”‚   â”‚   â”œâ”€â”€ session_memory.py
â”‚   â”‚   â”‚   â””â”€â”€ resource_store.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ doc_processing_unit/
â”‚   â”‚   â”œâ”€â”€ text_extractor.py
â”‚   â”‚   â”œâ”€â”€ text_cleaner.py
â”‚   â”‚   â”œâ”€â”€ chunking.py
â”‚   â”‚   â”œâ”€â”€ embedding_engine.py
â”‚   â”‚   â”œâ”€â”€ model_manager.py
â”‚   â”‚   â””â”€â”€ qdrant_manager.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€agent/
â”‚   â”‚       â”œâ”€â”€ graph_state.py
â”‚   â”‚       â”œâ”€â”€ rag_tool.py
â”‚   â”‚       â”œâ”€â”€ nodes/
â”‚   â”‚       â”‚   â”œâ”€â”€ assistant_node.py
â”‚   â”‚       â”‚   â”œâ”€â”€ finalize_node.py
â”‚   â”‚       â”‚   â””â”€â”€ tool_node.py
â”‚   â”‚       â””â”€â”€ graph_builder.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ uploads/
â”‚   â”‚   â””â”€â”€ processed/
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ file_manager.py
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â”‚
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ upload_section.py
â”‚   â”‚   â”œâ”€â”€ chat_section.py
â”‚   â”‚   â””â”€â”€ citation_box.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ api_client.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”‚
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_cleaner.py
â”‚   â”œâ”€â”€ test_chunking.py
â”‚   â”œâ”€â”€ test_model.py
â”‚   â”œâ”€â”€ test_qdrant.py
â”‚   â”œâ”€â”€ test_llm.py
â”‚   â”œâ”€â”€ test_rag_pipeline.py
â”‚   â”œâ”€â”€ test_embeddings.py
â”‚   â””â”€â”€ etc.
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```
---

# âš™ï¸ Tech Stack (Phase 2)

| Layer | Technology |
|--------------|-------------|
| **Agent** | Framework |	LangGraph |
| **LLM**	| Google Gemini 2.5 Flash |
| **Vector DB**	| Qdrant |
| **Embeddings**	| BAAI/bge-small-en-v1.5 |
| **Backend**	| FastAPI |
| **Frontend**	| Streamlit |
| **Memory**	| Sliding window via session_memory |

---

# ğŸ› ï¸ Installation & Setup
**1ï¸âƒ£ Clone Repository**
```bash
git clone https://github.com/Gauravmupase09/RAG-APP-PHASE2.git
cd RAG-APP-PHASE2
```
---

# ğŸ”§ Backend Setup (FastAPI)
**2ï¸âƒ£ Create Virtual Environment**
```bash
cd backend
python -m venv venv
venv/Scripts/activate
```
**3ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

**4ï¸âƒ£ Start Qdrant (Docker)**
```bash
docker run -p 6333:6333 qdrant/qdrant
```

**5ï¸âƒ£ Launch FastAPI Server**
```bash
uvicorn main:app --reload
```

API available at:
 - http://localhost:8000
 - http://localhost:8000/docs

---

# ğŸ¨ Frontend Setup (Streamlit)
```bash
cd ../frontend
python -m venv venv
venv/Scripts/activate
pip install -r requirements.txt
streamlit run app.py
```

Frontend:
ğŸ‘‰ http://localhost:8501

---

# ğŸ”„ Agentic Workflow (Detailed)
**1ï¸âƒ£ User sends a query**
- The system forwards it to assistant_node.

**2ï¸âƒ£ assistant_node decides:**
- If retrieval is needed â†’ calls rag_tool
- If it's general â†’ skips retrieval

**3ï¸âƒ£ tool_node retrieves:**
- top-k chunks
- citations
- returns structured payload

**4ï¸âƒ£ finalize_node creates final answer using:**
- session memory
- retrieved chunks (if any)
- formatted citations

Final output is written into `state.final_output`.

---

# ğŸ“¡ API Endpoints

| Method | Route	| Purpose |
|--------------|-------------|-------------|
| **POST** |	/api/upload	| Upload documents |
| **POST** | /api/process/{session_id}	| Process + embed documents |
| **POST** | /api/query	| Run Agentic RAG |
| **GET** | /api/list_docs	| List documents |
| **POST** | /api/reset_session	| Clear session + memory |

---

# ğŸ“š Example Agentic Behavior

**User:**
`Who are you?`
LLM decision: general mode â†’ no tool call

User:
`What does the document say about student expectations?`
LLM decision: retrieval required â†’ rag_tool â†’ RAG answer

---

# ğŸ§ª Tests Included

Covers:
  - extraction
  - cleaning
  - chunking
  - embeddings
  - Qdrant
  - LLM engine
  - RAG pipeline
  - LangGraph agent behavior

---

# ğŸ¤ Contributing

Contributions welcome!
You can propose:
- Multi-tool agent workflows
- More evaluators
- Streaming support
- Multi-document reasoning

---

# ğŸ“œ License

MIT License




