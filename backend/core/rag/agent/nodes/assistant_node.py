# backend/core/rag/agent/nodes/assistant_node.py

from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.runnables import RunnableConfig

from backend.core.rag.agent.graph_state import AgentState
from backend.core.rag.agent.rag_tool import rag_tool
from backend.core.rag.llm_engine import get_llm  # Your Gemini Flash 2.5 wrapper


# ============================================================
# üß† SYSTEM PROMPT FOR THE ASSISTANT NODE
# ============================================================

ASSISTANT_SYSTEM_PROMPT = """
You are an Intent Classification Controller for a Document Question Answering System.

Your ONLY responsibility is to decide whether the user‚Äôs question REQUIRES calling the `rag_tool`.

VERY IMPORTANT:
- You must NOT generate the final answer for the user.
- You must NOT summarize or explain anything.
- You must NOT fabricate document content.
- Another component will produce the final answer. Your job is ONLY to route correctly.

------------------------------------------------------------
WHEN YOU MUST CALL `rag_tool`:
------------------------------------------------------------
Call the tool if the user‚Äôs question requires ANY information that may exist inside the uploaded documents.

    - The query references uploaded files, PDFs, reports, sections, rules, tables, or content found inside documents.
    - The user says things like:
        "According to the document..."
        "What does the PDF say about..."
        "Explain section 3..."
        "Summarize findings in the uploaded file..."
    - If the answer needs grounding in the document, ALWAYS call `rag_tool`.

------------------------------------------------------------
WHEN YOU MUST NOT CALL `rag_tool`:
------------------------------------------------------------
Do NOT call the tool for:
    - General knowledge questions.
    - Small-talk or chit-chat ("hi", "hello", "who are you").
    - Reasoning questions that do NOT depend on document content
    - Personal questions
    - Questions that do NOT depend on the uploaded documents.

------------------------------------------------------------
AVAILABLE INFO:
------------------------------------------------------------
    - session_id: identifies the user's session
    - docs: list of uploaded document names (may be empty)

------------------------------------------------------------
YOUR OUTPUT MUST BE ONE OF:
------------------------------------------------------------

1Ô∏è‚É£ If document info IS required:
    ‚Üí Produce a tool call to `rag_tool`.

2Ô∏è‚É£ If document info is NOT required:
    ‚Üí Output an assistant message: "NO_TOOL_REQUIRED"

Another system component will produce the final answer.
"""


# ============================================================
# ü§ñ ASSISTANT NODE LOGIC
# ============================================================

async def assistant_node(state: AgentState, config: RunnableConfig):
    """
    Decides:
        - Should the query trigger rag_tool?
        - Or is it a general query?

    Output is ALWAYS an AIMessage:
        - AIMessage(tool_calls=[...])   ‚Üí tool execution path
        - AIMessage("NO_TOOL_REQUIRED") ‚Üí normal answering path
    """

    # 1Ô∏è‚É£ Load base LLM
    llm = get_llm()  # Your Gemini Flash 2.5 or any model you use

    # 2Ô∏è‚É£ Bind the tool ‚Äî now LLM can output tool call messages
    llm_with_tools = llm.bind_tools([rag_tool])

    # 3Ô∏è‚É£ Get latest user message
    user_msg = state["messages"][-1]

    # 4Ô∏è‚É£ Prepare docs metadata (MUST be merged into system prompt ‚Äî no extra SystemMessage allowed)
    docs = state.get("docs") or []
    docs_text = (
        f"Uploaded documents in this session: {', '.join(docs)}"
        if docs else
        "No uploaded documents found in this session."
    )

    session_text = f"session_id for this conversation: {state['session_id']}"

    # 5Ô∏è‚É£ Build FINAL system prompt given to Gemini
    SYSTEM_PROMPT = ASSISTANT_SYSTEM_PROMPT + f"\n\nSESSION METADATA:\n" + docs_text + "\n" + session_text

    # 6Ô∏è‚É£ Build classification prompt
    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=user_msg.content),
    ]

    # 7Ô∏è‚É£ Run LLM to classify
    response = await llm_with_tools.ainvoke(messages, config=config)

    # 8Ô∏è‚É£ If model did NOT call tool ‚Üí classify as general query
    if not getattr(response, "tool_calls", None):
        response = AIMessage(content="NO_TOOL_REQUIRED")

    # 9Ô∏è‚É£ Add decision result to state
    updated_messages = state["messages"] + [response]

    # Return updated state to LangGraph
    return {**state, "messages": updated_messages}